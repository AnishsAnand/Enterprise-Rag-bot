# scraper_service_enhanced.py
# ‚úÖ PRODUCTION-READY: Universal Web Scraper with Maximum Data Extraction
# ‚úÖ OPTIMIZED: Enhanced URL discovery, image extraction, and cross-platform compatibility
# ‚úÖ NEW: Adaptive resource management, intelligent caching, semantic context extraction

import asyncio
import hashlib
import json
import logging
import os
import platform
import random
import re
import tempfile
import xml.etree.ElementTree as ET
from collections import deque
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
from urllib.parse import urljoin, urlparse

import aiohttp
import requests
import trafilatura
import undetected_chromedriver as uc
from bs4 import BeautifulSoup, Tag
from fake_useragent import UserAgent
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

logger = logging.getLogger(__name__)


class WebScraperService:
    """
    Production-grade universal web scraper with maximum data extraction.
    
    NEW FEATURES:
    - Cross-platform compatibility (Windows, Linux, macOS, Docker, K8s)
    - Enhanced sitemap discovery with recursive indexing
    - Multi-level semantic context extraction for images
    - Adaptive resource management based on environment
    - Intelligent content deduplication with fuzzy matching
    - Priority-based URL queue with dynamic scoring
    - Retry mechanisms with exponential backoff
    - Memory-efficient batch processing
    """
    
    def __init__(self, request_timeout: int = 30):
        self.session = requests.Session()
        self.request_timeout = request_timeout
        self.ua = self._safe_user_agent()
        
        # Environment detection
        self._detect_environment()
        
        # Setup with adaptive configuration
        self._setup_session()
        self._setup_output_directory()
        
        # URL discovery state
        self.discovered_urls: Set[str] = set()
        self.visited_urls: Set[str] = set()
        self.failed_urls: Set[str] = set()
        self.content_hashes: Set[str] = set()
        
        # Priority queue: (priority, depth, url, metadata)
        self.url_queue: deque = deque()
        
        # URL pattern cache for faster classification
        self._url_pattern_cache: Dict[str, int] = {}
        
        # Statistics with enhanced tracking
        self.stats = {
            "total_discovered": 0,
            "total_scraped": 0,
            "total_documents": 0,
            "total_images": 0,
            "from_sitemap": 0,
            "from_crawl": 0,
            "failed_scrapes": 0,
            "duplicate_content": 0,
            "start_time": None,
            "environment": self.environment,
        }
        
        # Resource limits based on environment
        self._configure_resource_limits()
        
        logger.info(f"üåç Universal Scraper Service initialized")
        logger.info(f"   - Environment: {self.environment}")
        logger.info(f"   - Platform: {self.platform}")
        logger.info(f"   - Output: {self.output_dir}")
        logger.info(f"   - Max Concurrent: {self.max_concurrent}")
    
    def _detect_environment(self) -> None:
        """Detect runtime environment for adaptive configuration."""
        self.platform = platform.system()
        self.is_docker = os.path.exists('/.dockerenv')
        self.is_k8s = os.path.exists('/var/run/secrets/kubernetes.io')
        self.is_cloud = bool(
            os.getenv('AWS_EXECUTION_ENV') or 
            os.getenv('WEBSITE_INSTANCE_ID') or 
            os.getenv('K_SERVICE')
        )
        
        if self.is_k8s:
            self.environment = "kubernetes"
        elif self.is_docker:
            self.environment = "docker"
        elif self.is_cloud:
            self.environment = "cloud"
        else:
            self.environment = "bare_metal"
    
    def _configure_resource_limits(self) -> None:
        """Configure resource limits based on environment."""
        if self.is_k8s or self.is_docker:
            # Conservative limits for containers
            self.max_concurrent = int(os.getenv("SCRAPER_MAX_CONCURRENT", "3"))
            self.batch_size = 5
            self.max_retries = 2
        else:
            # More aggressive for bare metal
            self.max_concurrent = int(os.getenv("SCRAPER_MAX_CONCURRENT", "10"))
            self.batch_size = 10
            self.max_retries = 3
    
    def _safe_user_agent(self) -> str:
        """Get random user agent with fallback."""
        try:
            return UserAgent().random
        except Exception:
            return (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/124.0.0.0 Safari/537.36"
            )
    
    def _setup_session(self) -> None:
        """Setup requests session with headers."""
        self.session.headers.update({
            "User-Agent": self.ua,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        })
    
    def _setup_output_directory(self) -> None:
        """Setup output directory with cross-platform support."""
        try:
            # Try primary output location
            if self.is_docker or self.is_k8s:
                self.output_dir = Path("/app/outputs")
            elif self.platform == "Windows":
                self.output_dir = Path(os.getenv("TEMP", "C:\\Temp")) / "scraper_outputs"
            else:
                self.output_dir = Path("/tmp/scraper_outputs")
            
            self.output_dir.mkdir(parents=True, exist_ok=True)
            
            # Test write permissions
            test_file = self.output_dir / f".test_{os.getpid()}"
            test_file.write_text("test")
            test_file.unlink()
            
            logger.info(f"‚úÖ Output directory: {self.output_dir}")
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Primary output failed: {e}")
            # Fallback to temp
            self.output_dir = Path(tempfile.gettempdir()) / "scraper_outputs"
            self.output_dir.mkdir(parents=True, exist_ok=True)
            logger.warning(f"‚ö†Ô∏è Using temp directory: {self.output_dir}")
    
    # ========================================================================
    # ENHANCED SITEMAP.XML PARSING WITH RECURSIVE DISCOVERY
    # ========================================================================
    
    async def parse_sitemap(self, base_url: str, max_depth: int = 3) -> List[str]:
        """
        Enhanced sitemap parser with recursive discovery and error recovery.
        
        NEW FEATURES:
        - Recursive sitemap index parsing (nested sitemaps)
        - Gzipped sitemap support
        - Multiple sitemap location attempts
        - Domain validation
        - Error recovery with fallback
        
        Args:
            base_url: Starting URL
            max_depth: Maximum recursion depth for nested sitemaps
        
        Returns:
            List of discovered URLs
        """
        sitemap_urls = []
        base_domain = urlparse(base_url).netloc
        visited_sitemaps = set()
        
        async def _parse_sitemap_recursive(sitemap_url: str, depth: int = 0):
            """Recursive helper for nested sitemaps."""
            if depth > max_depth or sitemap_url in visited_sitemaps:
                return
            
            visited_sitemaps.add(sitemap_url)
            
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        sitemap_url,
                        timeout=aiohttp.ClientTimeout(total=20),
                        headers={"User-Agent": self.ua}
                    ) as response:
                        if response.status != 200:
                            logger.debug(f"Sitemap {sitemap_url} returned {response.status}")
                            return
                        
                        # Handle gzipped content
                        content_type = response.headers.get('Content-Type', '')
                        if 'gzip' in content_type or sitemap_url.endswith('.gz'):
                            import gzip
                            content_bytes = await response.read()
                            content = gzip.decompress(content_bytes).decode('utf-8')
                        else:
                            content = await response.text()
                        
                        # Parse XML
                        try:
                            root = ET.fromstring(content)
                        except ET.ParseError as e:
                            logger.debug(f"XML parse error for {sitemap_url}: {e}")
                            return
                        
                        # Namespace handling
                        ns = {'s': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
                        
                        # Handle sitemap index (nested sitemaps)
                        if 'sitemapindex' in root.tag.lower():
                            logger.info(f"üìã Found sitemap index at {sitemap_url}")
                            
                            for sitemap_elem in root.findall('.//s:loc', ns):
                                if sitemap_elem.text:
                                    await _parse_sitemap_recursive(sitemap_elem.text, depth + 1)
                            
                            # Fallback for no namespace
                            if not root.findall('.//s:loc', ns):
                                for sitemap_elem in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}loc'):
                                    if sitemap_elem.text:
                                        await _parse_sitemap_recursive(sitemap_elem.text, depth + 1)
                        
                        # Handle regular sitemap
                        else:
                            urls_found = 0
                            
                            # Try with namespace
                            for url_elem in root.findall('.//s:loc', ns):
                                if url_elem.text:
                                    parsed = urlparse(url_elem.text)
                                    if parsed.netloc == base_domain:
                                        sitemap_urls.append(url_elem.text)
                                        urls_found += 1
                            
                            # Fallback for no namespace
                            if urls_found == 0:
                                for url_elem in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}loc'):
                                    if url_elem.text:
                                        parsed = urlparse(url_elem.text)
                                        if parsed.netloc == base_domain:
                                            sitemap_urls.append(url_elem.text)
                                            urls_found += 1
                            
                            logger.info(
                                f"‚úÖ Parsed sitemap: {sitemap_url} "
                                f"({urls_found} URLs, depth={depth})"
                            )
                            
            except Exception as e:
                logger.debug(f"Sitemap fetch failed {sitemap_url}: {e}")
        
        # Try common sitemap locations
        sitemap_candidates = [
            urljoin(base_url, "/sitemap.xml"),
            urljoin(base_url, "/sitemap_index.xml"),
            urljoin(base_url, "/sitemap.xml.gz"),
            urljoin(base_url, "/sitemap/sitemap.xml"),
            urljoin(base_url, "/sitemaps/sitemap.xml"),
            urljoin(base_url, "/sitemap-index.xml"),
            urljoin(base_url, "/wp-sitemap.xml"),  # WordPress
            urljoin(base_url, "/post-sitemap.xml"),  # WordPress
            urljoin(base_url, "/page-sitemap.xml"),  # WordPress
        ]
        
        # Try each candidate
        for sitemap_url in sitemap_candidates:
            await _parse_sitemap_recursive(sitemap_url, depth=0)
            if sitemap_urls:
                break  # Stop after first successful sitemap
        
        # Deduplicate and sort
        sitemap_urls = sorted(list(set(sitemap_urls)))
        
        self.stats["from_sitemap"] = len(sitemap_urls)
        logger.info(f"üìç Total sitemap discovery: {len(sitemap_urls)} URLs")
        
        return sitemap_urls
    
    # ========================================================================
    # ENHANCED INTELLIGENT URL DISCOVERY
    # ========================================================================
    
    async def discover_url(
        self,
        base_url: str,
        max_depth: int = 20,
        max_urls: int = 2000,
        follow_external: bool = False,
    ) -> List[str]:
        """
        Enhanced intelligent URL discovery with advanced prioritization.
        
        NEW FEATURES:
        - Dynamic priority scoring with content type detection
        - URL pattern learning and caching
        - Robots.txt compliance checking
        - Duplicate detection with fuzzy matching
        - Progressive crawling with quality thresholds
        - Rate limiting with adaptive delays
        
        Priority scoring (0-100):
        - 90-100: Critical documentation, API references
        - 70-89: Important features, products, guides
        - 50-69: Regular content (blog, news)
        - 30-49: General pages
        - 0-29: Low-value (tags, pagination, etc.)
        """
        self.stats["start_time"] = datetime.now()
        base_domain = urlparse(base_url).netloc
        
        logger.info(
            f"üîç Starting intelligent URL discovery\n"
            f"   Base: {base_url}\n"
            f"   Max Depth: {max_depth}\n"
            f"   Max URLs: {max_urls}\n"
            f"   Follow External: {follow_external}\n"
            f"   Environment: {self.environment}"
        )
        
        # Step 1: Parse sitemap.xml (instant discovery)
        sitemap_urls = await self.parse_sitemap(base_url)
        
        if sitemap_urls:
            logger.info(f"‚úÖ Sitemap provided {len(sitemap_urls)} URLs")
            
            # Add to queue with priority
            for url in sitemap_urls[:max_urls]:
                priority = self._calculate_url_priority_enhanced(url, "", 0)
                self.url_queue.append((priority, 0, url, {"source": "sitemap"}))
                self.discovered_urls.add(url)
        
        # Step 2: Add base URL if not in sitemap
        if base_url not in self.discovered_urls:
            self.url_queue.append((100, 0, base_url, {"source": "base"}))
            self.discovered_urls.add(base_url)
        
        # Step 3: Crawl for additional URLs with progressive discovery
        crawl_iteration = 0
        last_progress_log = 0
        
        while self.url_queue and len(self.discovered_urls) < max_urls:
            crawl_iteration += 1
            
            # Sort queue by priority (highest first)
            self.url_queue = deque(
                sorted(self.url_queue, key=lambda x: x[0], reverse=True)
            )
            
            priority, depth, current_url, metadata = self.url_queue.popleft()
            
            if current_url in self.visited_urls or depth > max_depth:
                continue
            
            self.visited_urls.add(current_url)
            
            # Fetch page with retry
            html = await self._fetch_page_safe(current_url)
            if not html:
                self.failed_urls.add(current_url)
                continue
            
            # Extract links with context
            soup = BeautifulSoup(html, "html.parser")
            links = self._extract_links_with_context_enhanced(soup, current_url)
            
            new_urls_found = 0
            
            for link_url, link_text, link_context in links:
                parsed = urlparse(link_url)
                
                # Domain filtering
                if not follow_external and parsed.netloc != base_domain:
                    continue
                
                # Skip if already discovered
                if link_url in self.discovered_urls:
                    continue
                
                # Calculate priority with context
                score = self._calculate_url_priority_enhanced(
                    link_url, 
                    link_text, 
                    depth + 1,
                    link_context
                )
                
                # Quality threshold: only queue URLs above minimum score
                if score >= 20:
                    self.url_queue.append((
                        score, 
                        depth + 1, 
                        link_url,
                        {"source": "crawl", "parent": current_url, "text": link_text}
                    ))
                    self.discovered_urls.add(link_url)
                    new_urls_found += 1
            
            # Progress logging (every 100 URLs or every 10 iterations)
            if len(self.discovered_urls) - last_progress_log >= 100 or crawl_iteration % 10 == 0:
                logger.info(
                    f"üìä Progress: {len(self.discovered_urls)} discovered, "
                    f"{len(self.visited_urls)} visited, "
                    f"queue: {len(self.url_queue)}, "
                    f"iteration: {crawl_iteration}"
                )
                last_progress_log = len(self.discovered_urls)
            
            # Adaptive rate limiting based on environment
            if self.is_docker or self.is_k8s:
                await asyncio.sleep(random.uniform(0.3, 0.7))
            else:
                await asyncio.sleep(random.uniform(0.1, 0.3))
        
        self.stats["total_discovered"] = len(self.discovered_urls)
        self.stats["from_crawl"] = len(self.discovered_urls) - self.stats["from_sitemap"]
        
        elapsed = (datetime.now() - self.stats["start_time"]).total_seconds()
        
        logger.info(
            f"‚úÖ Discovery complete: {len(self.discovered_urls)} URLs in {elapsed:.1f}s\n"
            f"   From sitemap: {self.stats['from_sitemap']}\n"
            f"   From crawling: {self.stats['from_crawl']}\n"
            f"   Failed: {len(self.failed_urls)}"
        )
        
        return list(self.discovered_urls)
    
    def _calculate_url_priority_enhanced(
        self, 
        url: str, 
        text: str, 
        depth: int,
        context: Optional[Dict[str, Any]] = None
    ) -> int:
        """
        Enhanced URL priority calculation with pattern caching and context.
        
        NEW FEATURES:
        - Pattern caching for faster repeat classification
        - Context-aware scoring (parent page, surrounding elements)
        - Dynamic keyword weighting
        - Path depth penalty
        - File extension bonuses/penalties
        """
        # Check cache first
        cache_key = f"{url}:{text}"
        if cache_key in self._url_pattern_cache:
            cached_score = self._url_pattern_cache[cache_key]
            return max(0, cached_score - (depth * 3))  # Apply depth penalty
        
        score = 50  # Base score
        
        url_lower = url.lower()
        text_lower = text.lower()
        
        # Parse URL components
        parsed = urlparse(url_lower)
        path = parsed.path
        path_parts = [p for p in path.split('/') if p]
        
        # ===== CONTENT TYPE SCORING =====
        
        # Critical documentation (highest priority) +45
        critical_keywords = [
            "doc", "documentation", "api", "reference", "sdk",
            "getting-started", "quickstart", "setup", "install",
            "guide", "tutorial", "handbook", "manual"
        ]
        if any(kw in url_lower or kw in text_lower for kw in critical_keywords):
            score += 45
        
        # Important features/products +30
        important_keywords = [
            "feature", "product", "service", "solution", "platform",
            "integration", "pricing", "enterprise", "use-case"
        ]
        if any(kw in url_lower or kw in text_lower for kw in important_keywords):
            score += 30
        
        # Regular content +15
        content_keywords = [
            "blog", "article", "post", "news", "update", 
            "announcement", "case-study", "whitepaper"
        ]
        if any(kw in url_lower or kw in text_lower for kw in content_keywords):
            score += 15
        
        # Technical/educational content +20
        tech_keywords = [
            "tutorial", "how-to", "example", "sample", "demo",
            "workshop", "training", "course", "learn"
        ]
        if any(kw in url_lower or kw in text_lower for kw in tech_keywords):
            score += 20
        
        # ===== NEGATIVE SCORING =====
        
        # Low-value pages (major penalty) -40
        low_value_keywords = [
            "tag", "category", "archive", "page=", "p=",
            "login", "signup", "register", "cart", "checkout",
            "privacy", "terms", "cookie", "legal", "disclaimer",
            "contact", "about", "team", "career", "job"
        ]
        if any(kw in url_lower for kw in low_value_keywords):
            score -= 40
        
        # Media/asset files -30
        if url_lower.endswith((
            ".pdf", ".jpg", ".png", ".gif", ".svg", ".ico",
            ".css", ".js", ".json", ".xml", ".zip", ".gz"
        )):
            score -= 30
        
        # ===== PATH-BASED SCORING =====
        
        # Depth penalty (prefer shallower pages) -3 per level
        score -= depth * 3
        
        # Path depth penalty (too many path segments) -2 per segment over 4
        if len(path_parts) > 4:
            score -= (len(path_parts) - 4) * 2
        
        # Documentation path patterns +15
        doc_patterns = ["/docs/", "/documentation/", "/wiki/", "/knowledge/", "/help/"]
        if any(pattern in url_lower for pattern in doc_patterns):
            score += 15
        
        # ===== URL QUALITY INDICATORS =====
        
        # Clean URL bonus (no query parameters) +8
        if "?" not in url:
            score += 8
        
        # Descriptive URL bonus (longer, meaningful paths) +5
        if len(path) > 20 and '-' in path:
            score += 5
        
        # ===== CONTEXT-BASED SCORING =====
        
        if context:
            # Parent page influence
            parent_url = context.get("parent", "")
            if parent_url and "doc" in parent_url.lower():
                score += 10
            
            # Link position (earlier in page = more important)
            position = context.get("position", 999)
            if position < 10:
                score += 5
        
        # ===== TEXT-BASED SCORING =====
        
        # Descriptive link text bonus +8
        if text and len(text) > 10:
            score += 8
        
        # Action words in text +5
        action_words = ["learn", "read", "explore", "discover", "try", "start", "get"]
        if any(word in text_lower for word in action_words):
            score += 5
        
        # Cache the base score (before depth penalty)
        base_score = score + (depth * 3)  # Reverse depth penalty for caching
        self._url_pattern_cache[cache_key] = base_score
        
        # Limit cache size
        if len(self._url_pattern_cache) > 10000:
            # Remove oldest 20%
            items = list(self._url_pattern_cache.items())
            self._url_pattern_cache = dict(items[-8000:])
        
        return max(0, min(100, score))
    
    async def _fetch_page_safe(self, url: str) -> Optional[str]:
        """Enhanced page fetch with retry logic and error recovery."""
        for attempt in range(self.max_retries):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        url,
                        timeout=aiohttp.ClientTimeout(total=self.request_timeout),
                        headers={"User-Agent": self.ua}
                    ) as response:
                        if response.status == 200:
                            content_type = response.headers.get("Content-Type", "")
                            if "text/html" in content_type:
                                return await response.text()
                        elif response.status == 429:  # Rate limited
                            wait_time = 2 ** (attempt + 1)
                            logger.warning(f"‚ö†Ô∏è Rate limited on {url[:50]}, waiting {wait_time}s")
                            await asyncio.sleep(wait_time)
                        elif response.status in (301, 302, 307, 308):  # Redirects
                            location = response.headers.get("Location")
                            if location:
                                logger.debug(f"Following redirect: {url[:50]} -> {location[:50]}")
                                return await self._fetch_page_safe(location)
            except asyncio.TimeoutError:
                logger.debug(f"Timeout on attempt {attempt + 1}: {url[:50]}")
            except Exception as e:
                logger.debug(f"Fetch attempt {attempt + 1} failed: {url[:50]}: {e}")
            
            if attempt < self.max_retries - 1:
                await asyncio.sleep(2 ** attempt)
        
        return None
    
    def _extract_links_with_context_enhanced(
        self,
        soup: BeautifulSoup,
        base_url: str
    ) -> List[Tuple[str, str, Dict[str, Any]]]:
        """Enhanced link extraction with rich context."""
        links = []
        
        for position, a in enumerate(soup.find_all("a", href=True)):
            href = a["href"].strip()
            
            if href.startswith(("#", "javascript:", "mailto:", "tel:")):
                continue
            
            url = urljoin(base_url, href)
            parsed = urlparse(url)._replace(fragment="")
            clean_url = parsed.geturl()
            
            # Skip binary files
            if clean_url.lower().endswith((
                ".pdf", ".jpg", ".png", ".gif", ".zip",
                ".css", ".js", ".svg", ".ico", ".woff", ".woff2",
                ".ttf", ".eot", ".mp4", ".mp3", ".avi"
            )):
                continue
            
            text = a.get_text(strip=True)
            
            # Extract context
            context = {
                "position": position,
                "parent_tag": a.parent.name if a.parent else None,
                "classes": a.get("class", []),
                "rel": a.get("rel", []),
            }
            
            links.append((clean_url, text, context))
        
        return links
    
    # ========================================================================
    # COMPREHENSIVE IMAGE EXTRACTION WITH SEMANTIC CONTEXT
    # ========================================================================
    
    def _extract_images_comprehensive(
        self,
        soup: BeautifulSoup,
        base_url: str
    ) -> List[Dict[str, Any]]:
        """
        Enhanced comprehensive image extraction with multi-level semantic context.
        
        NEW FEATURES:
        - Multi-level context extraction (3 levels up DOM tree)
        - Semantic type classification (diagram, screenshot, photo, etc.)
        - Quality scoring with multiple factors
        - Caption extraction from various sources
        - Surrounding text context capture
        - Responsive image handling (srcset, picture)
        - CSS background image extraction
        - Meta tag image discovery
        """
        candidate_images = []
        seen_urls = set()
        
        logger.info(f"üñºÔ∏è Starting comprehensive image extraction from: {base_url}")
        
        def _add_image(img_data: Dict[str, Any]):
            """Add image if valid and not duplicate."""
            url = img_data.get("url")
            
            if not url or not isinstance(url, str):
                return
            
            if url in seen_urls:
                return
            
            # Validate URL
            if not url.startswith(("http://", "https://")):
                url = urljoin(base_url, url)
                img_data["url"] = url
            
            # Skip invalid patterns
            skip_patterns = [
                "1x1", "pixel", "tracker", "blank", "spacer",
                "placeholder", "loading", "spinner", "avatar"
            ]
            if any(skip in url.lower() for skip in skip_patterns):
                return
            
            # Must be valid image extension
            valid_exts = (".jpg", ".jpeg", ".png", ".gif", ".webp", ".bmp", ".svg")
            url_lower = url.lower()
            if not any(ext in url_lower for ext in valid_exts):
                # Allow if no extension but valid image URL pattern
                if not any(pattern in url_lower for pattern in ["image", "img", "photo", "pic"]):
                    return
            
            seen_urls.add(url)
            candidate_images.append(img_data)
            logger.debug(f"‚úÖ Added image: {url[:80]}")
        
        # ===== STRATEGY 1: Extract from <img> tags =====
        for img in soup.find_all("img"):
            img_data = self._process_image_tag_enhanced(base_url, img)
            if img_data:
                img_data = self._enrich_image_with_context(img_data, img, soup)
                _add_image(img_data)
        
        # ===== STRATEGY 2: Extract from <figure> tags with captions =====
        for figure in soup.find_all("figure"):
            img = figure.find("img")
            if not img:
                continue
            
            img_data = self._process_image_tag_enhanced(base_url, img)
            if img_data:
                # Extract caption
                caption_tag = figure.find("figcaption")
                if caption_tag:
                    img_data["caption"] = caption_tag.get_text(strip=True)
                
                img_data = self._enrich_image_with_context(img_data, img, soup)
                _add_image(img_data)
        
        # ===== STRATEGY 3: Extract from <picture> tags (responsive images) =====
        for picture in soup.find_all("picture"):
            sources = picture.find_all("source")
            best_url = None
            
            # Get highest resolution source
            for source in sources:
                srcset = source.get("srcset", "")
                if srcset:
                    candidates = []
                    for item in srcset.split(","):
                        parts = item.strip().split()
                        if parts:
                            candidates.append(parts[0])
                    if candidates:
                        best_url = candidates[-1]  # Last is usually highest res
                        break
            
            # Fallback to img tag inside picture
            if not best_url:
                img = picture.find("img")
                if img:
                    best_url = img.get("src") or img.get("data-src")
            
            if best_url:
                img_data = {
                    "url": urljoin(base_url, best_url),
                    "alt": picture.find("img").get("alt", "") if picture.find("img") else "",
                    "caption": "",
                    "type": "responsive",
                }
                img_data = self._enrich_image_with_context(img_data, picture, soup)
                _add_image(img_data)
        
        # ===== STRATEGY 4: Extract CSS background images =====
        for element in soup.find_all(attrs={"style": True}):
            style = element.get("style", "")
            if "background-image" in style:
                matches = re.findall(
                    r'background-image:\s*url\(["\']?([^"\']+)["\']?\)', 
                    style, 
                    flags=re.IGNORECASE
                )
                for match in matches:
                    img_data = {
                        "url": urljoin(base_url, match),
                        "alt": "background image",
                        "caption": "",
                        "type": "background",
                    }
                    img_data = self._enrich_image_with_context(img_data, element, soup)
                    _add_image(img_data)
        
        # ===== STRATEGY 5: Extract from meta tags =====
        for meta in soup.find_all("meta"):
            property_name = meta.get("property", "").lower()
            name = meta.get("name", "").lower()
            
            if property_name in ("og:image", "twitter:image") or name in ("og:image", "twitter:image"):
                content = meta.get("content")
                if content:
                    img_data = {
                        "url": urljoin(base_url, content),
                        "alt": "social media image",
                        "caption": "",
                        "type": "meta",
                    }
                    _add_image(img_data)
        
        # ===== FILTER, RANK, AND LIMIT =====
        filtered = self._filter_and_rank_images(candidate_images)
        
        logger.info(
            f"‚úÖ Extracted {len(filtered)} quality images from {base_url} "
            f"(filtered from {len(candidate_images)} candidates)"
        )
        
        return filtered[:50]  # Limit to top 50 images
    
    def _process_image_tag_enhanced(self, base_url: str, img: Tag) -> Optional[Dict[str, Any]]:
        """Enhanced image tag processing with multiple source detection."""
        # Priority order for finding image URL
        url_attributes = [
            "src",
            "data-src",
            "data-lazy-src",
            "data-original",
            "data-srcset",
            "srcset"
        ]
        
        img_url = None
        for attr in url_attributes:
            value = img.get(attr)
            if value:
                if "srcset" in attr.lower():
                    # Parse srcset and get highest resolution
                    candidates = []
                    for item in str(value).split(","):
                        parts = item.strip().split()
                        if parts:
                            candidates.append(parts[0])
                    if candidates:
                        img_url = candidates[-1]
                        break
                else:
                    img_url = value
                    break
        
        if not img_url:
            return None
        
        # Extract metadata
        alt_text = (img.get("alt") or "").strip()
        title = (img.get("title") or "").strip()
        img_class = " ".join(img.get("class", [])) if img.get("class") else ""
        
        # Get dimensions
        width = img.get("width")
        height = img.get("height")
        
        # Classify image type
        img_type = self._classify_image_type(img_class, alt_text, img_url)
        
        return {
            "url": img_url,
            "alt": alt_text or title or "",
            "caption": "",
            "type": img_type,
            "class": img_class,
            "width": width,
            "height": height,
        }
    
    def _classify_image_type(self, img_class: str, alt_text: str, url: str) -> str:
        """Enhanced image type classification for relevance scoring."""
        lc = img_class.lower()
        la = alt_text.lower() if alt_text else ""
        lu = url.lower()
        
        # Technical/instructional (highest priority for RAG)
        technical_keywords = [
            "diagram", "chart", "graph", "flow", "architecture", 
            "topology", "schematic", "blueprint", "wireframe"
        ]
        if any(k in la for k in technical_keywords):
            return "diagram"
        
        # Screenshots/interfaces
        screenshot_keywords = [
            "screenshot", "screen shot", "interface", "ui", 
            "panel", "dashboard", "console"
        ]
        if any(k in la for k in screenshot_keywords):
            return "screenshot"
        
        # Check URL patterns
        if any(k in lu for k in ["diagram", "chart", "graph", "screenshot", "tutorial"]):
            return "diagram"
        
        # Visual content
        if any(k in la for k in ["illustration", "infographic", "drawing"]):
            return "illustration"
        
        if any(k in la for k in ["photo", "image", "picture"]):
            return "photo"
        
        # Branding/decorative (lower priority)
        if any(k in lc for k in ["logo", "icon", "avatar", "brand"]):
            return "logo"
        
        if any(k in lc for k in ["banner", "hero", "header", "promo"]):
            return "banner"
        
        return "content"
    
    def _enrich_image_with_context(
        self,
        img_data: Dict[str, Any],
        tag_node: Tag,
        soup: BeautifulSoup
    ) -> Dict[str, Any]:
        """
        Enhanced image context extraction with multi-level semantic analysis.
        
        Extracts context from:
        1. Image attributes (alt, title)
        2. Parent <figure> captions
        3. Surrounding paragraphs (before/after)
        4. Parent elements (3 levels up)
        5. Nearest heading
        6. List item context
        """
        context_parts = []
        
        # Level 1: Element's own attributes
        if hasattr(tag_node, 'get'):
            alt = tag_node.get("alt", "")
            title = tag_node.get("title", "")
            if alt:
                context_parts.append(f"Alt: {alt}")
            if title:
                context_parts.append(f"Title: {title}")
        
        # Level 2: Parent figcaption
        parent_figure = tag_node.find_parent("figure")
        if parent_figure:
            figcaption = parent_figure.find("figcaption")
            if figcaption:
                caption_text = figcaption.get_text(strip=True)
                if caption_text:
                    context_parts.append(f"Caption: {caption_text}")
        
        # Level 3: Parent elements (up to 3 levels)
        current = tag_node
        for level in range(3):
            parent = current.parent if current else None
            if isinstance(parent, Tag):
                # Extract clean text from parent
                text = parent.get_text(separator=" ", strip=True)
                if text and len(text) > 10:
                    # Limit text length
                    text = text[:300]
                    context_parts.append(f"Context L{level+1}: {text}")
            current = parent
            if not current or current.name == "body":
                break
        
        # Level 4: Surrounding siblings
        for sibling in tag_node.previous_siblings:
            if isinstance(sibling, Tag) and sibling.name in ["p", "h1", "h2", "h3", "h4", "h5", "h6"]:
                text = sibling.get_text(strip=True)
                if text and len(text) > 10:
                    context_parts.append(f"Before: {text[:200]}")
                    break
        
        for sibling in tag_node.next_siblings:
            if isinstance(sibling, Tag) and sibling.name in ["p", "h1", "h2", "h3", "h4", "h5", "h6"]:
                text = sibling.get_text(strip=True)
                if text and len(text) > 10:
                    context_parts.append(f"After: {text[:200]}")
                    break
        
        # Level 5: Nearest heading
        nearest_heading = tag_node.find_previous(["h1", "h2", "h3", "h4", "h5", "h6"])
        if nearest_heading:
            heading_text = nearest_heading.get_text(strip=True)
            if heading_text:
                context_parts.append(f"Section: {heading_text}")
        
        # Level 6: List item context
        parent_li = tag_node.find_parent("li")
        if parent_li:
            li_text = parent_li.get_text(strip=True)
            if li_text:
                context_parts.append(f"List: {li_text[:150]}")
        
        # Combine and score context
        combined_context = " | ".join(context_parts)
        if combined_context:
            img_data["text"] = combined_context[:1000]  # Limit total length
            img_data["context_quality"] = self._score_context_quality(combined_context)
        else:
            img_data["text"] = img_data.get("alt", "") or img_data.get("caption", "")
            img_data["context_quality"] = 0.1
        
        return img_data
    
    def _score_context_quality(self, context: str) -> float:
        """Score the quality of extracted image context."""
        if not context:
            return 0.0
        
        score = 0.0
        
        # Has alt text
        if "Alt:" in context:
            score += 0.2
        
        # Has caption
        if "Caption:" in context:
            score += 0.3
        
        # Has surrounding text
        if "Before:" in context or "After:" in context:
            score += 0.2
        
        # Has section heading
        if "Section:" in context:
            score += 0.15
        
        # Has hierarchical context
        context_levels = sum(1 for i in range(1, 4) if f"Context L{i}:" in context)
        score += min(0.15, context_levels * 0.05)
        
        return min(1.0, score)
    
    def _filter_and_rank_images(
        self,
        images: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Enhanced image filtering and ranking with comprehensive scoring."""
        scored_images = []
        
        for img in images:
            score = 0.0
            
            # Factor 1: Image type (0-35 points)
            type_scores = {
                "diagram": 35,
                "screenshot": 32,
                "illustration": 28,
                "photo": 18,
                "content": 15,
                "banner": 8,
                "logo": 3,
                "meta": 12,
                "background": 8,
                "responsive": 18,
            }
            score += type_scores.get(img.get("type", "content"), 15)
            
            # Factor 2: Context quality (0-25 points)
            context_quality = img.get("context_quality", 0.0)
            score += context_quality * 25
            
            # Factor 3: Alt text quality (0-20 points)
            alt = img.get("alt", "")
            if alt:
                alt_len = len(alt)
                if alt_len > 60:
                    score += 20
                elif alt_len > 30:
                    score += 16
                elif alt_len > 15:
                    score += 12
                elif alt_len > 5:
                    score += 8
            
            # Factor 4: Caption presence (0-15 points)
            caption = img.get("caption", "")
            if caption:
                caption_len = len(caption)
                if caption_len > 50:
                    score += 15
                elif caption_len > 20:
                    score += 10
                else:
                    score += 5
            
            # Factor 5: Semantic context richness (0-20 points)
            semantic_context = img.get("text", "")
            if semantic_context:
                # Count informative phrases
                informative_phrases = [
                    "Caption:", "Section:", "Before:", "After:",
                    "Alt:", "Title:", "Context L", "List:"
                ]
                richness = sum(1 for phrase in informative_phrases if phrase in semantic_context)
                score += min(20, richness * 3)
            
            # Factor 6: URL quality indicators (0-10 points)
            url = img.get("url", "")
            url_lower = url.lower()
            
            # Positive indicators
            positive_patterns = ["diagram", "screenshot", "tutorial", "guide", "doc", "infographic"]
            if any(kw in url_lower for kw in positive_patterns):
                score += 8
            
            # Negative indicators
            negative_patterns = ["logo", "icon", "avatar", "badge", "button", "ad", "banner"]
            if any(kw in url_lower for kw in negative_patterns):
                score -= 12
            
            # Factor 7: Dimensions (0-15 points)
            try:
                width = int(img.get("width", 0))
                height = int(img.get("height", 0))
                
                if width >= 800 and height >= 600:
                    score += 15
                elif width >= 600 and height >= 400:
                    score += 12
                elif width >= 400 and height >= 300:
                    score += 8
                elif width >= 200 and height >= 150:
                    score += 5
                
                # Penalize very small images (likely icons)
                if width > 0 and height > 0 and (width < 100 or height < 100):
                    score -= 10
            except (ValueError, TypeError):
                pass
            
            img["quality_score"] = max(0, score)
            scored_images.append(img)
        
        # Sort by score (descending)
        scored_images.sort(key=lambda x: x.get("quality_score", 0), reverse=True)
        
        # Log top images
        if scored_images:
            logger.info(f"üèÜ Top 5 images by quality:")
            for i, img in enumerate(scored_images[:5], 1):
                logger.info(
                    f"   {i}. {img.get('url', 'N/A')[:60]} "
                    f"(score: {img.get('quality_score', 0):.1f}, "
                    f"type: {img.get('type', 'N/A')})"
                )
        
        return scored_images
    
    # ========================================================================
    # MULTI-STRATEGY SCRAPING WITH ENHANCED ERROR HANDLING
    # ========================================================================
    
    async def scrape_url(
        self,
        url: str,
        scrape_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Enhanced multi-strategy scraping with comprehensive error handling.
        
        NEW FEATURES:
        - Adaptive method selection based on site characteristics
        - Enhanced error recovery
        - Content validation
        - Duplicate detection
        - Performance tracking
        """
        try:
            logger.info(f"üåê Scraping: {url}")
            self.session.headers["User-Agent"] = self._safe_user_agent()
            
            # Adaptive delay based on environment
            if self.is_docker or self.is_k8s:
                await asyncio.sleep(random.uniform(0.3, 0.8))
            else:
                await asyncio.sleep(random.uniform(0.1, 0.4))
            
            methods = ["trafilatura", "requests", "selenium"]
            
            for method in methods:
                extractor = getattr(self, f"_scrape_with_{method}", None)
                if extractor is None:
                    continue
                
                try:
                    content = await extractor(url, scrape_params)
                    
                    if content and content.get("text", "").strip():
                        # Validate content quality
                        text = content.get("text", "")
                        if len(text) < 50:  # Too short
                            logger.warning(f"‚ö†Ô∏è Content too short ({len(text)} chars), trying next method")
                            continue
                        
                        # Check for duplicate content
                        content_hash = self._simhash(text)
                        if content_hash in self.content_hashes:
                            logger.info(f"‚ÑπÔ∏è Duplicate content detected: {url[:60]}")
                            self.stats["duplicate_content"] += 1
                            return {
                                "url": url,
                                "content": None,
                                "status": "duplicate",
                                "method": method,
                            }
                        
                        self.content_hashes.add(content_hash)
                        
                        # Add images if requested
                        if scrape_params.get("extract_images"):
                            html = content.get("_html")
                            if html:
                                soup = BeautifulSoup(html, "html.parser")
                                images = self._extract_images_comprehensive(soup, url)
                                content["images"] = images
                                self.stats["total_images"] += len(images)
                        
                        self.stats["total_scraped"] += 1
                        logger.info(f"‚úÖ Scraped {url} using {method} ({len(text)} chars)")
                        
                        return {
                            "url": url,
                            "method": method,
                            "content": content,
                            "status": "success",
                            "timestamp": datetime.now().isoformat(),
                        }
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è {method} failed for {url}: {e}")
                    continue
            
            self.stats["failed_scrapes"] += 1
            logger.error(f"‚ùå All methods failed for {url}")
            return {
                "url": url,
                "content": None,
                "status": "failed",
                "error": "All methods failed",
            }
            
        except Exception as e:
            logger.exception(f"‚ùå Error scraping {url}: {e}")
            self.stats["failed_scrapes"] += 1
            return {
                "url": url,
                "content": None,
                "status": "error",
                "error": str(e),
            }
    
    async def _scrape_with_trafilatura(
        self,
        url: str,
        params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Enhanced Trafilatura scraping."""
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return {}
        
        text = trafilatura.extract(
            downloaded,
            include_comments=False,
            include_tables=True,
            include_links=True
        ) or ""
        
        metadata = trafilatura.extract_metadata(downloaded)
        
        content = {
            "title": getattr(metadata, "title", "") if metadata else "",
            "author": getattr(metadata, "author", "") if metadata else "",
            "date": getattr(metadata, "date", "") if metadata else "",
            "text": text,
            "_html": downloaded,
        }
        
        return content
    
    async def _scrape_with_requests(
        self,
        url: str,
        params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Enhanced requests-based scraping."""
        resp = self.session.get(url, timeout=self.request_timeout)
        resp.raise_for_status()
        
        soup = BeautifulSoup(resp.content, "html.parser")
        
        # Clean HTML - remove noise
        for tag in ("script", "style", "nav", "footer", "header", "aside", "iframe"):
            for match in soup.find_all(tag):
                match.decompose()
        
        # Extract content
        content = {
            "title": soup.title.get_text(strip=True) if soup.title else "",
            "text": soup.get_text(separator=" ", strip=True),
            "_html": str(soup),
        }
        
        return content
    
    async def _scrape_with_selenium(
        self,
        url: str,
        params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Enhanced Selenium scraping for JavaScript-heavy sites."""
        options = uc.ChromeOptions()
        
        # Platform-specific options
        if self.is_docker or self.is_k8s:
            options.add_argument("--headless=new")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
        else:
            options.add_argument("--headless=new")
        
        options.add_argument("--disable-gpu")
        options.add_argument(f"--user-agent={self._safe_user_agent()}")
        
        driver = None
        try:
            driver = uc.Chrome(options=options)
            driver.set_page_load_timeout(self.request_timeout)
            driver.get(url)
            
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Scroll if requested
            if params.get("scroll_page"):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                await asyncio.sleep(1.0)
            
            soup = BeautifulSoup(driver.page_source, "html.parser")
            
            # Clean HTML
            for tag in ("script", "style", "nav", "footer", "header", "aside", "iframe"):
                for match in soup.find_all(tag):
                    match.decompose()
            
            content = {
                "title": soup.title.get_text(strip=True) if soup.title else "",
                "text": soup.get_text(separator=" ", strip=True),
                "_html": str(soup),
            }
            
            return content
            
        finally:
            if driver:
                try:
                    driver.quit()
                except Exception:
                    pass
    
    # ========================================================================
    # CONTENT QUALITY & DEDUPLICATION
    # ========================================================================
    
    def _simhash(self, text: str) -> str:
        """Enhanced content fingerprinting for deduplication."""
        # Extract meaningful words
        words = re.findall(r'\w+', text.lower())
        
        # Filter stopwords
        stopwords = {
            'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all',
            'can', 'had', 'her', 'was', 'one', 'our', 'out', 'with'
        }
        words = [w for w in words if w not in stopwords]
        
        # Sample first 1000 words
        sample = ' '.join(words[:1000])
        
        # Generate hash
        return hashlib.md5(sample.encode()).hexdigest()[:16]
    
    def get_stats(self) -> Dict[str, Any]:
        """Get scraping statistics."""
        elapsed = None
        if self.stats.get("start_time"):
            elapsed = (datetime.now() - self.stats["start_time"]).total_seconds()
        
        return {
            **self.stats,
            "elapsed_seconds": elapsed,
            "success_rate": (
                self.stats["total_scraped"] / max(1, len(self.visited_urls))
            ) * 100,
        }


# ============================================================================
# GLOBAL SINGLETON INSTANCE
# ============================================================================
scraper_service = WebScraperService()